# Model Configuration Update - llama31-8b-steady:latest

## Summary

The installation script and configuration files have been updated to use **`llama31-8b-steady:latest`** as the default conversational model instead of the previous default.

## Changes Made

### 1. Installation Script Default
**File**: [selo-ai/install-complete.sh:173](selo-ai/install-complete.sh#L173)

**Changed**:
```bash
DEFAULT_CONVERSATIONAL_MODEL="llama31-8b-steady"
```

**To**:
```bash
DEFAULT_CONVERSATIONAL_MODEL="llama31-8b-steady:latest"
```

### 2. Backend Environment Example
**File**: [selo-ai/backend/.env.example:45](selo-ai/backend/.env.example#L45)

**Changed**:
```bash
CONVERSATIONAL_MODEL=llama31-8b-steady
```

**To**:
```bash
CONVERSATIONAL_MODEL=llama31-8b-steady:latest
```

### 3. Default Configuration Template
**File**: [selo-ai/configs/default/.env.template:18](selo-ai/configs/default/.env.template#L18)

**Changed**:
```bash
CONVERSATIONAL_MODEL=llama31-8b-steady
```

**To**:
```bash
CONVERSATIONAL_MODEL=llama31-8b-steady:latest
```

### 4. Model Installation Script
**File**: [selo-ai/configs/default/install-models.sh](selo-ai/configs/default/install-models.sh)

**Changed** (Line 65):
```bash
if ! pull_model "llama3:8b" "Conversational Model (LLaMA 3 8B)"; then
    FAILED_MODELS+=("llama3:8b")
fi
```

**To**:
```bash
if ! pull_model "llama31-8b-steady:latest" "Conversational Model (LLaMA 3.1 8B Steady)"; then
    FAILED_MODELS+=("llama31-8b-steady:latest")
fi
```

**Changed** (Line 92):
```bash
echo "  â€¢ llama3:8b          (Conversational)"
```

**To**:
```bash
echo "  â€¢ llama31-8b-steady:latest  (Conversational)"
```

---

## What This Means for Fresh Installation

When you run `install-complete.sh` for a fresh installation, the system will now:

âœ… **Set `llama31-8b-steady:latest`** as the default conversational model
âœ… **Pull `llama31-8b-steady:latest`** from Ollama during model installation
âœ… **Configure backend/.env** with the correct model name
âœ… **Use this model** for all chat/conversation interactions

---

## Combined with GPU Optimization

Your fresh installation will get both optimizations:

### 1. **Correct Model** (This Update):
```bash
CONVERSATIONAL_MODEL=llama31-8b-steady:latest
```

### 2. **Stable GPU Settings** (Previous Update):
```bash
OLLAMA_GPU_LAYERS=40              # 40 layers on GPU (not all 999)
CHAT_NUM_CTX=4096                 # 4K context window
TORCH_CUDA_MEMORY_FRACTION=0.75   # 75% GPU memory limit
OLLAMA_NUM_PARALLEL=2             # Concurrent requests
```

---

## Fresh Installation Command

Simply run the installation script as normal:

```bash
cd /mnt/local/Projects/SELODSP/selo-ai
bash install-complete.sh
```

The installer will automatically:
1. âœ… Configure `llama31-8b-steady:latest` as conversational model
2. âœ… Apply stable GPU settings (40 layers, 4096 context)
3. âœ… Pull the correct model from Ollama
4. âœ… Set up optimal configuration for 16GB GPU

---

## Model Specifications

**llama31-8b-steady:latest**:
- **Model**: LLaMA 3.1 8B (Steady variant)
- **Quantization**: Q8_0 (8-bit, high quality)
- **Size**: ~8.5 GB
- **Context**: 128K native (configured to 4096 for stability)
- **Use case**: Conversational AI, chat interface

**With 40 GPU layers**:
- **GPU memory**: ~7 GB
- **CPU offload**: ~1.5 GB
- **Performance**: ~95% of full GPU speed
- **Stability**: âœ… No OOM crashes

---

## Verification After Installation

To verify the model is configured correctly after installation:

```bash
# Check backend/.env has correct model:
grep CONVERSATIONAL_MODEL /mnt/local/Projects/SELODSP/selo-ai/backend/.env

# Expected output:
# CONVERSATIONAL_MODEL=llama31-8b-steady:latest

# Verify model is pulled in Ollama:
ollama list | grep llama31-8b-steady

# Expected output:
# llama31-8b-steady:latest  ...  8.5 GB  ...
```

---

## Model Installation Details

During installation, the script will:

1. **Detect model requirement**: `llama31-8b-steady:latest`
2. **Pull from Ollama registry**:
   ```
   ðŸ“¦ Conversational Model (LLaMA 3.1 8B Steady)
      Model: llama31-8b-steady:latest
      Downloading...
      âœ“ Successfully installed
   ```
3. **Configure in backend/.env**
4. **Warm up the model** for fast first response

---

## All Files Updated

âœ… [selo-ai/install-complete.sh](selo-ai/install-complete.sh#L173)
âœ… [selo-ai/backend/.env.example](selo-ai/backend/.env.example#L45)
âœ… [selo-ai/configs/default/.env.template](selo-ai/configs/default/.env.template#L18)
âœ… [selo-ai/configs/default/install-models.sh](selo-ai/configs/default/install-models.sh#L65)

---

## Summary

Your fresh installation will now automatically use:
- âœ… **Model**: `llama31-8b-steady:latest` (not llama3:8b)
- âœ… **GPU Layers**: 40 (not 999)
- âœ… **Context Window**: 4096 (not 8192)
- âœ… **Memory Limit**: 75% (4GB safety buffer)
- âœ… **Stability**: No OOM crashes under load

Ready for fresh installation! ðŸš€
