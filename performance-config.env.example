# =============================================================================
# SELO AI Performance Optimizations Configuration
# =============================================================================
# Copy this file to .env and adjust values for your deployment
# All settings are optional - defaults are production-ready

# =============================================================================
# HTTP Retry Configuration
# =============================================================================
# Number of HTTP retry attempts before falling back to subprocess
# Recommended: 3-5 for production, 1-2 for development
OLLAMA_HTTP_RETRIES=3

# Delay between HTTP retry attempts (seconds)
# Recommended: 0.5-1.0 for production, 0.1-0.3 for development
OLLAMA_HTTP_RETRY_DELAY=0.5

# =============================================================================
# Token Management & Context Window
# =============================================================================
# Context window size (total tokens for prompt + completion)
# Must match your model's context size
# - Small models (3B-7B): 2048-4096
# - Medium models (7B-13B): 4096-8192
# - Large models (13B+): 8192-32768
CHAT_NUM_CTX=4096

# Base max tokens for chat responses
# Router will clamp this if prompt is too long
# Recommended: 512-1024 for quick responses, 1024-2048 for detailed
CHAT_NUM_PREDICT=1024

# Base max tokens for reflection generation
# Optimized for rich narrative inner monologues (200-300 words)
# Previous default: 256 (limited depth)
# Recommended: 400-640 for richer narrative content
REFLECTION_NUM_PREDICT=400

# Base max tokens for analytical tasks (SDL, persona evolution)
# Needs space for structured JSON output
# Recommended: 1024-2048
ANALYTICAL_NUM_PREDICT=1024

# =============================================================================
# Conversation History Windowing
# =============================================================================
# Number of recent messages to keep in full detail
# Older messages will be summarized
# Recommended: 15-25 for most use cases
CHAT_HISTORY_WINDOW_SIZE=20

# Maximum tokens for entire conversation history
# Includes both raw recent messages and summary
# Recommended: 1/3 to 1/2 of CHAT_NUM_CTX
CHAT_HISTORY_MAX_TOKENS=2048

# =============================================================================
# Deferred Embeddings Configuration
# =============================================================================
# Number of reflection themes to process per batch
# Higher = more efficient but longer delay
# Recommended: 5-15 depending on hardware
EMBEDDING_BATCH_SIZE=10

# Interval between embedding batch processing (seconds)
# Lower = more responsive but higher overhead
# Recommended: 1.0-5.0 depending on load
EMBEDDING_PROCESS_INTERVAL=2.0

# =============================================================================
# Ollama Performance Tuning (Optional)
# =============================================================================
# Number of threads for Ollama inference
# Leave unset for auto-detection
# Recommended: Number of physical cores (not threads)
# OLLAMA_NUM_THREAD=8

# Number of GPU layers to offload
# Leave unset for auto-detection
# Set to 0 for CPU-only, -1 for full GPU
# OLLAMA_GPU_LAYERS=-1

# Number of GPUs to use
# -1 = auto-detect and use all available
# 0 = CPU only
# N = use N GPUs
OLLAMA_NUM_GPU=-1

# =============================================================================
# Chat-Specific LLM Parameters
# =============================================================================
# Temperature for chat responses (0.0-1.0)
# Lower = more focused, higher = more creative
# Recommended: 0.5-0.7 for conversation
CHAT_TEMPERATURE=0.6

# Top-K sampling parameter
# Number of top tokens to consider
# Recommended: 20-40
CHAT_TOP_K=40

# Top-P (nucleus) sampling parameter (0.0-1.0)
# Cumulative probability threshold
# Recommended: 0.85-0.95
CHAT_TOP_P=0.9

# =============================================================================
# Reflection-Specific LLM Parameters
# =============================================================================
# Temperature for reflection generation (0.0-1.0)
# Lower = more consistent, higher = more creative
# Previous default: 0.3 (very deterministic)
# Recommended: 0.35-0.45 for natural, expressive inner voice while maintaining structure
REFLECTION_TEMPERATURE=0.35

# =============================================================================
# Analytical Task LLM Parameters
# =============================================================================
# Temperature for analytical tasks (0.0-1.0)
# Very low for structured output consistency
# Recommended: 0.1-0.3
ANALYTICAL_TEMPERATURE=0.2

# =============================================================================
# Performance Profiles
# =============================================================================
# Uncomment one of the following profile sections to use preset configurations

# --- PROFILE: High Performance (Requires good hardware) ---
# OLLAMA_HTTP_RETRIES=3
# OLLAMA_HTTP_RETRY_DELAY=0.3
# CHAT_NUM_CTX=8192
# CHAT_NUM_PREDICT=2048
# REFLECTION_NUM_PREDICT=640
# ANALYTICAL_NUM_PREDICT=2048
# CHAT_HISTORY_WINDOW_SIZE=30
# CHAT_HISTORY_MAX_TOKENS=4096
# EMBEDDING_BATCH_SIZE=20
# EMBEDDING_PROCESS_INTERVAL=1.0
# OLLAMA_NUM_GPU=-1
# REFLECTION_TEMPERATURE=0.4
# REFLECTION_OUTPUT_STYLE=verbose

# --- PROFILE: Balanced (Default) ---
# OLLAMA_HTTP_RETRIES=3
# OLLAMA_HTTP_RETRY_DELAY=0.5
# CHAT_NUM_CTX=4096
# CHAT_NUM_PREDICT=1024
# REFLECTION_NUM_PREDICT=400
# ANALYTICAL_NUM_PREDICT=1024
# CHAT_HISTORY_WINDOW_SIZE=20
# CHAT_HISTORY_MAX_TOKENS=2048
# EMBEDDING_BATCH_SIZE=10
# EMBEDDING_PROCESS_INTERVAL=2.0
# REFLECTION_TEMPERATURE=0.35
# REFLECTION_OUTPUT_STYLE=verbose

# --- PROFILE: Low Resource (For limited hardware) ---
# OLLAMA_HTTP_RETRIES=2
# OLLAMA_HTTP_RETRY_DELAY=0.5
# CHAT_NUM_CTX=2048
# CHAT_NUM_PREDICT=512
# REFLECTION_NUM_PREDICT=256
# ANALYTICAL_NUM_PREDICT=512
# CHAT_HISTORY_WINDOW_SIZE=10
# CHAT_HISTORY_MAX_TOKENS=1024
# EMBEDDING_BATCH_SIZE=5
# EMBEDDING_PROCESS_INTERVAL=5.0
# OLLAMA_NUM_GPU=0
# REFLECTION_TEMPERATURE=0.32
# REFLECTION_OUTPUT_STYLE=hybrid

# =============================================================================
# Monitoring & Debugging
# =============================================================================
# Set to DEBUG to see detailed performance logs
# LOG_LEVEL=INFO

# =============================================================================
# Advanced Tuning
# =============================================================================
# Reflection sync mode (true = wait for reflection before responding)
# Set to false for async reflections (faster response, but no reflection in context)
REFLECTION_SYNC_MODE=true

# Reflection sync timeout (seconds)
# 0 or negative = unbounded (wait forever)
# Positive = timeout after N seconds and continue without reflection
REFLECTION_SYNC_TIMEOUT_S=180

# LLM request timeout (seconds)
# 0 or negative = unbounded
# Positive = timeout after N seconds
LLM_TIMEOUT=120

# =============================================================================
# Notes
# =============================================================================
# 1. Adaptive token clamping is automatic - the router will reduce max_tokens
#    if the prompt is too long, ensuring it fits in the context window
#
# 2. History windowing is applied automatically when messages exceed limits
#    Older messages are summarized to maintain context coherence
#
# 3. Deferred embeddings run in the background and don't block responses
#    Adjust batch size and interval based on your reflection generation rate
#
# 4. All retry logic is transparent - failed HTTP requests are retried
#    automatically before falling back to subprocess
#
# 5. Streaming is enabled by default and provides immediate token feedback
#    to users without waiting for complete responses
