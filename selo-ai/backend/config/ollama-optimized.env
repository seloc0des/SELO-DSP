# Ollama Optimized Configuration for SELO-DSP
# These settings fix the critical concurrency bottleneck
# 
# USAGE:
# 1. For systemd-managed Ollama:
#    sudo systemctl edit ollama
#    Add these under [Service]:
#    Environment="OLLAMA_NUM_PARALLEL=2"
#    Environment="OLLAMA_KEEP_ALIVE=30m"
#    sudo systemctl restart ollama
#
# 2. For manual Ollama:
#    export OLLAMA_NUM_PARALLEL=2
#    export OLLAMA_KEEP_ALIVE=30m
#    ollama serve

# CRITICAL: Enable concurrent model execution
# Allows reflection (qwen2.5:3b) and chat (llama3:8b) to run simultaneously
# Without this, all requests queue serially causing 5+ minute delays
OLLAMA_NUM_PARALLEL=2

# Keep models loaded in VRAM for 30 minutes to avoid reload overhead
# Your RTX 4060 Ti has 15.6GB - enough for both models simultaneously
OLLAMA_KEEP_ALIVE=30m

# Context length for better quality
OLLAMA_CONTEXT_LENGTH=8192

# GPU configuration
OLLAMA_NUM_GPU=1

# Thread count - adjust based on your CPU cores
# Check with: nproc
OLLAMA_NUM_THREAD=8

# Flash attention can improve performance on newer GPUs
OLLAMA_FLASH_ATTENTION=true

# Max queue size for pending requests
OLLAMA_MAX_QUEUE=512

# Don't limit loaded models - let VRAM be the constraint
OLLAMA_MAX_LOADED_MODELS=0
