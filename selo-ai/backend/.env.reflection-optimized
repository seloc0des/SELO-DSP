# ============================================================================
# REFLECTION QUALITY OPTIMIZATION CONFIGURATION
# ============================================================================
# Copy these settings to your backend/.env file to enable richer narrative
# content in inner reflections while maintaining generation speed.
#
# Based on audit findings from REFLECTION_GENERATION_AUDIT.md
# Date: October 23, 2025
# ============================================================================

# === Token Budget Optimization ===
# Optimized for 170-500 word reflections (target 250-350 words)
# This uses ~20% of available context for rich narrative depth
REFLECTION_NUM_PREDICT=540
REFLECTION_MAX_TOKENS=540

# === Temperature Tuning ===
# Increased from 0.3 to 0.35 for more natural, expressive language
# Still maintains consistency while allowing varied vocabulary
REFLECTION_TEMPERATURE=0.35

# === Output Style Control ===
# Explicit verbose mode: 2-3 paragraphs, 180-320 words
# Blends concrete moments, emotional texture, forward-looking commitments
REFLECTION_OUTPUT_STYLE=verbose

# === Context Window ===
# qwen2.5:3b native capacity - all tiers now use full 8192 token window
CHAT_NUM_CTX=8192

# === Timeout Configuration ===
# Keep these at 0 (unbounded) for quality generation
# LLM completes thoughts naturally without artificial cutoffs
LLM_TIMEOUT=0
REFLECTION_LLM_TIMEOUT_S=0
REFLECTION_SYNC_TIMEOUT_S=0

# ============================================================================
# OPTIONAL: HIGH-PERFORMANCE TIER PROFILE
# ============================================================================
# These values are auto-detected and set by installers for 12GB+ GPUs
# Supports full 170-650 word range with enhanced depth
#
# REFLECTION_NUM_PREDICT=650
# REFLECTION_MAX_TOKENS=650
# REFLECTION_WORD_MAX=650
# REFLECTION_TEMPERATURE=0.35
# ANALYTICAL_NUM_PREDICT=1536
# CHAT_NUM_PREDICT=2048
# CHAT_NUM_CTX=8192  # Same for both tiers - uses model's native capacity

# ============================================================================
# MONITORING RECOMMENDATIONS
# ============================================================================
# After applying these changes, monitor:
# 1. Reflection schema compliance rate (should stay >95%)
# 2. Average generation time (may increase 10-20%)
# 3. Content richness (look for better narrative flow)
# 4. Memory usage (minimal increase expected)
# 5. Retry frequency (should remain low)
#
# Adjust token limits and temperature based on observed quality vs. speed
# trade-offs for your specific use case.
