# Backend environment example for SELO AI
# Copy to .env and adjust values as needed for your environment.

# Async SQLAlchemy URL for PostgreSQL (local default)
# Format: postgresql+asyncpg://USER:PASS@HOST/DBNAME
DATABASE_URL=postgresql+asyncpg://seloai:password@localhost/seloai

# API key for securing endpoints (change in production)
SELO_SYSTEM_API_KEY=dev-secret-key-change-me

# Ollama configuration
OLLAMA_BASE_URL=http://127.0.0.1:11434

# Models (installer default set)
# The Linux installer (install-complete.sh) writes these into backend/.env using
# the configs/default template. You can override them after installation if needed.
# Historical tiered profiles (ultra/light/heavy) are kept only for migration/audit
# and are no longer exposed as separate user-facing profiles.
CONVERSATIONAL_MODEL=llama3:8b
ANALYTICAL_MODEL=qwen2.5:3b
REFLECTION_LLM=qwen2.5:3b

# Backend server settings
HOST=0.0.0.0
PORT=8000

# CORS for the frontend URL
CORS_ORIGINS=http://localhost:3000

# Embedding configuration
EMBEDDING_MODEL=nomic-embed-text
EMBEDDING_DIM=2048

# === Reflection-first and generation budgets (bounded defaults) ===
# Keep reflections synchronous but respect bounded backend timeouts
REFLECTION_ENFORCE_NO_TIMEOUTS=false
REFLECTION_SYNC_MODE=sync
REFLECTION_LLM_TIMEOUT_S=0
REFLECTION_SYNC_TIMEOUT_S=0
LLM_TIMEOUT=0
REFLECTION_REQUIRED=true

# === LLM Sampling Parameters ===
# Temperature controls randomness/creativity (0.0 = deterministic, 1.0 = very creative)
# Chat: moderate creativity for natural conversation
CHAT_TEMPERATURE=0.6

# Reflection: slightly higher than previous 0.3 for more natural, expressive inner voice
# while maintaining consistency for structured output
REFLECTION_TEMPERATURE=0.35

# Analytical: low temperature for consistent structured output
ANALYTICAL_TEMPERATURE=0.2

# === Reflection Output Style ===
# Options: verbose (2-3 paragraphs, 180-320 words), hybrid, concise
# Verbose mode enables rich narrative with emotional texture and context
REFLECTION_OUTPUT_STYLE=verbose

# === Token Budgets for Quality Generation ===
# Auto-detection (GPU tier) sets concise defaults:
#
# STANDARD TIER (<12GB GPU - optimized for 8GB):
#   REFLECTION_NUM_PREDICT=480
#   REFLECTION_MAX_TOKENS=480
#   REFLECTION_WORD_MAX=180
#   ANALYTICAL_NUM_PREDICT=640
#   CHAT_NUM_CTX=8192
#
# HIGH-PERFORMANCE TIER (â‰¥12GB GPU):
#   REFLECTION_NUM_PREDICT=480
#   REFLECTION_MAX_TOKENS=480
#   REFLECTION_WORD_MAX=180
#   ANALYTICAL_NUM_PREDICT=1536
#   CHAT_NUM_CTX=8192
#
# If unset, Python falls back to these tier-aware defaults automatically.
# Override below only when you need custom budgets.
#
# Chat completions: keep responses under the hard cap while allowing adaptivity.
CHAT_RESPONSE_MAX_TOKENS=320
# Optional predictor override (set 0 to let router manage budgets dynamically).
CHAT_NUM_PREDICT=0

# Reflection generation token budget
# Set to 0 for unbounded (prompt-constrained). Installer can override.
REFLECTION_NUM_PREDICT=0
REFLECTION_MAX_TOKENS=0
# Word count limits adjusted based on LLM comparison testing (Dec 2025)
# Models (qwen2.5:1.5b, qwen2.5:3b) naturally generate 180-200 word reflections
# Range of 90-180 keeps reflections creative yet concise
REFLECTION_WORD_MIN=90
REFLECTION_WORD_MAX=180

# Analytical model budget for structured outputs (traits, seed generation)
# Standard: 640 tokens, High-tier: 1536 tokens (prevents JSON truncation)
ANALYTICAL_NUM_PREDICT=640

# Context window size - qwen2.5:3b native capacity
# Both tiers use 8192 tokens (full model capacity)
CHAT_NUM_CTX=8192

# === Socket.IO Configuration ===
# Socket.IO ping timeout (seconds) - how long to wait for pong before considering connection dead
# Default: 60 seconds, Production: 86400 (24 hours) for backgrounded browser tabs
SOCKETIO_PING_TIMEOUT=60
# Socket.IO ping interval (seconds) - how often to send ping packets
# Default: 25 seconds
SOCKETIO_PING_INTERVAL=25

# === GPU/CUDA Configuration ===
# CUDA device selection (0 for first GPU, 1 for second, etc.)
CUDA_VISIBLE_DEVICES=0
# CUDA device ordering
CUDA_DEVICE_ORDER=PCI_BUS_ID
# PyTorch CUDA memory management
PYTORCH_CUDA_ALLOC_CONF=max_split_size_mb:128,expandable_segments:True
# GPU memory fraction for PyTorch (0.0-1.0)
TORCH_CUDA_MEMORY_FRACTION=0.8
# Disable CUDA launch blocking for performance
CUDA_LAUNCH_BLOCKING=0

